---
---

@article{xia2023spatial,
  abbr={NatComm},
  title={Spatial-linked alignment tool (SLAT) for aligning heterogenous slices},
  author={Chen-Rui, Xia* and Zhi-Jie, Cao*, XinMing, Tu and Ge Gao†},
  journal={Nature Communications},
  volume={14},
  number={1},
  pages={7236},
  year={2023},
  abstract={Spatially resolved omics technologies reveal the spatial organization of cells in various biological systems. Here we propose SLAT (Spatially-Linked Alignment Tool), a graph-based algorithm for efficient and effective alignment of spatial slices. Adopting a graph adversarial matching strategy, SLAT is the first algorithm capable of aligning heterogenous spatial data across distinct technologies and modalities. Systematic benchmarks demonstrate SLAT's superior precision, robustness, and speed over existing state-of-the-arts. Applications to multiple real-world datasets further show SLAT's utility in enhancing cell-typing resolution, integrating multiple modalities for regulatory inference, and mapping fine-scale spatial-temporal changes during development. The full SLAT package is available at https://github.com/gao-lab/SLAT.},
  html={https://www.nature.com/articles/s41467-023-43105-5},
  pdf={https://www.nature.com/articles/s41467-023-43105-5.pdf},
  publisher={Nature Publishing Group UK London},
  selected={true},
  reward={Editors' Highlight}
}


@article{tu2022cross,
  abbr={NeurIPS},
  title={Cross-linked unified embedding for cross-modality representation learning},
  author={XinMing, Tu*, Zhi-Jie, Cao*, Chen-Rui, Xia, Sara Mostafavi† and Ge Gao†},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={15942--15955},
  abstract={Multi-modal learning is essential for understanding information in the real world. Jointly learning from multi-modal data enables global integration of both shared and modality-specific information, but current strategies often fail when observa- tions from certain modalities are incomplete or missing for part of the subjects. To learn comprehensive representations based on such modality-incomplete data, we present a semi-supervised neural network model called CLUE (Cross-Linked Unified Embedding). Extending from multi-modal VAEs, CLUE introduces the use of cross-encoders to construct latent representations from modality-incomplete observations. Representation learning for modality-incomplete observations is common in genomics. For example, human cells are tightly regulated across multi- ple related but distinct modalities such as DNA, RNA, and protein, jointly defining a cell’s function. We benchmark CLUE on multi-modal data from single cell measurements, illustrating CLUE’s superior performance in all assessed categories of the NeurIPS 2021 Multimodal Single-cell Data Integration Competition. While we focus on analysis of single cell genomic datasets, we note that the proposed cross-linked embedding strategy could be readily applied to other cross-modality representation learning problems.},
  html={https://proceedings.neurips.cc/paper_files/paper/2022/hash/662b1774ba8845fc1fa3d1fc0177ceeb-Abstract-Conference.html},
  pdf={https://proceedings.neurips.cc/paper_files/paper/2022/file/662b1774ba8845fc1fa3d1fc0177ceeb-Paper-Conference.pdf},
  year={2022},
  selected={true},
  reward={Oral}
}